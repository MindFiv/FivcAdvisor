# Strands â†” LangChain API æ˜ å°„å‚è€ƒ

---

## ğŸ“¦ åŒ…çº§åˆ«æ˜ å°„

| Strands | LangChain | è¯´æ˜ |
|---------|-----------|------|
| `strands.agent.Agent` | `langchain.agents.AgentExecutor` | ä¸»è¦ Agent ç±» |
| `strands.multiagent.Swarm` | `langgraph.graph.StateGraph` | å¤šæ™ºèƒ½ä½“ç¼–æ’ |
| `strands.models.Model` | `langchain_core.language_models.LLM` | åŸºç¡€æ¨¡å‹æ¥å£ |
| `strands.types.tools.AgentTool` | `langchain_core.tools.Tool` | å·¥å…·å®šä¹‰ |
| `strands.hooks.HookRegistry` | è‡ªå®ç° | äº‹ä»¶ç³»ç»Ÿ |

---

## ğŸ¤– Agent ç±»æ˜ å°„

### åˆ›å»º
```python
# Strands
agent = Agent(
    model=model,
    tools=tools,
    system_prompt="You are...",
    name="MyAgent",
    agent_id="agent-1",
    callback_handler=handler,
    conversation_manager=conv_mgr,
    hooks=[hook1, hook2]
)

# LangChain
from langchain.agents import create_react_agent
from langchain.agents import AgentExecutor

agent = create_react_agent(
    llm=model,
    tools=tools,
    system_prompt="You are..."
)
executor = AgentExecutor.from_agent_and_tools(
    agent=agent,
    tools=tools,
    # éœ€è¦è‡ªå®ç°: callback_handler, hooks
)
```

### æ–¹æ³•æ˜ å°„

| Strands | LangChain | è¯´æ˜ |
|---------|-----------|------|
| `agent.invoke(query)` | `executor.invoke({"input": query})` | åŒæ­¥è°ƒç”¨ |
| `agent.invoke_async(query)` | `executor.ainvoke({"input": query})` | å¼‚æ­¥è°ƒç”¨ |
| `agent.structured_output_async(schema, prompt)` | éœ€è¦è‡ªå®ç° | ç»“æ„åŒ–è¾“å‡º |
| `agent.stream(query)` | `executor.stream({"input": query})` | æµå¼è¾“å‡º |

---

## ğŸ› ï¸ å·¥å…·ç³»ç»Ÿæ˜ å°„

### å·¥å…·å®šä¹‰
```python
# Strands
from strands.tools import tool

@tool
def calculator(expression: str) -> float:
    """Calculate mathematical expression"""
    return eval(expression)

# LangChain
from langchain_core.tools import tool

@tool
def calculator(expression: str) -> float:
    """Calculate mathematical expression"""
    return eval(expression)
```

### å·¥å…·åˆ—è¡¨
```python
# Strands
tools = [tool1, tool2, tool3]
agent = Agent(tools=tools, ...)

# LangChain
tools = [tool1, tool2, tool3]
agent = create_react_agent(llm, tools)
```

---

## ğŸ§  æ¨¡å‹æ˜ å°„

### OpenAI
```python
# Strands
from strands.models.openai import OpenAIModel
model = OpenAIModel(
    model_id="gpt-4",
    client_args={"api_key": "..."},
    params={"temperature": 0.5}
)

# LangChain
from langchain_openai import ChatOpenAI
model = ChatOpenAI(
    model="gpt-4",
    api_key="...",
    temperature=0.5
)
```

### Ollama
```python
# Strands
from strands.models.ollama import OllamaModel
model = OllamaModel(
    base_url="http://localhost:8000",
    model_id="llama2",
    temperature=0.5
)

# LangChain
from langchain_community.llms import Ollama
model = Ollama(
    base_url="http://localhost:8000",
    model="llama2",
    temperature=0.5
)
```

### LiteLLM
```python
# Strands
from strands.models.litellm import LiteLLMModel
model = LiteLLMModel(
    model_id="gpt-4",
    params={"api_key": "...", "temperature": 0.5}
)

# LangChain
from langchain_community.llms import LiteLLM
model = LiteLLM(
    model="gpt-4",
    api_key="...",
    temperature=0.5
)
```

---

## ğŸ“¨ æ¶ˆæ¯ç±»å‹æ˜ å°„

| Strands | LangChain | è¯´æ˜ |
|---------|-----------|------|
| `Message` | `BaseMessage` | åŸºç¡€æ¶ˆæ¯ |
| `ToolUse` | `ToolCall` | å·¥å…·è°ƒç”¨ |
| `ToolResult` | `ToolMessage` | å·¥å…·ç»“æœ |
| `StreamEvent` | `StreamEvent` | æµäº‹ä»¶ |

### æ¶ˆæ¯è½¬æ¢
```python
# Strands Message
from strands.types.content import Message
msg = Message(role="user", content="Hello")

# LangChain BaseMessage
from langchain_core.messages import HumanMessage
msg = HumanMessage(content="Hello")
```

---

## ğŸ”— å¤šæ™ºèƒ½ä½“æ˜ å°„

### Swarm
```python
# Strands
from strands.multiagent import Swarm
swarm = Swarm(agents=[agent1, agent2, agent3])
result = await swarm.invoke_async(query)

# LangChain (LangGraph)
from langgraph.graph import StateGraph
from langgraph.graph import END

graph = StateGraph(AgentState)
graph.add_node("agent1", agent1_node)
graph.add_node("agent2", agent2_node)
graph.add_edge("agent1", "agent2")
graph.add_edge("agent2", END)
app = graph.compile()
result = await app.ainvoke({"input": query})
```

---

## ğŸ£ Hook/äº‹ä»¶ç³»ç»Ÿæ˜ å°„

### Hook æ³¨å†Œ
```python
# Strands
from strands.hooks import HookRegistry
hooks = HookRegistry()
hooks.register(BeforeInvocationEvent, callback)

# LangChain (è‡ªå®ç°)
event_bus = EventBus()
event_bus.on("before_invocation", callback)
```

### äº‹ä»¶ç±»å‹æ˜ å°„

| Strands | LangChain æ›¿ä»£ |
|---------|---------------|
| `BeforeInvocationEvent` | `before_invocation` |
| `AfterInvocationEvent` | `after_invocation` |
| `MessageAddedEvent` | `message_added` |
| `AgentInitializedEvent` | `agent_initialized` |

---

## ğŸ’¾ å¯¹è¯ç®¡ç†æ˜ å°„

### å¯¹è¯å†å²
```python
# Strands
from strands.agent import SlidingWindowConversationManager
conv_mgr = SlidingWindowConversationManager(max_tokens=2000)
agent = Agent(conversation_manager=conv_mgr, ...)

# LangChain
from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(max_token_limit=2000)
# é›†æˆåˆ° AgentExecutor
```

---

## ğŸ”„ å›è°ƒå¤„ç†æ˜ å°„

### å›è°ƒå¤„ç†å™¨
```python
# Strands
class MyCallbackHandler:
    def __call__(self, **kwargs):
        if "event" in kwargs:
            # å¤„ç†æµäº‹ä»¶
        if "message" in kwargs:
            # å¤„ç†æ¶ˆæ¯äº‹ä»¶

agent = Agent(callback_handler=MyCallbackHandler(), ...)

# LangChain
from langchain_core.callbacks import BaseCallbackHandler

class MyCallbackHandler(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        pass
    def on_llm_end(self, response, **kwargs):
        pass

executor = AgentExecutor.from_agent_and_tools(
    agent=agent,
    tools=tools,
    callbacks=[MyCallbackHandler()]
)
```

---

## ğŸ“Š ç±»å‹ç³»ç»Ÿæ˜ å°„

| Strands | LangChain |
|---------|-----------|
| `AgentResult` | `dict` æˆ– `AgentAction` |
| `Status` enum | è‡ªå®šä¹‰ enum |
| `StreamEvent` | `StreamEvent` |


